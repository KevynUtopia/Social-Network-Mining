{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP4332PA2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbHGxCOqw6na",
        "outputId": "ebfbb6f9-7365-4cf0-c381-e60f82332bab"
      },
      "source": [
        "!pip install gensim\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WWhfhtRxCFW"
      },
      "source": [
        "def load_data(file_name):\n",
        "    \"\"\"\n",
        "    read edges from an edge file\n",
        "    \"\"\"\n",
        "    edges = list()\n",
        "    df = pd.read_csv(file_name)\n",
        "    for idx, row in df.iterrows():\n",
        "        user_id, friends = row[\"user_id\"], eval(row[\"friends\"])\n",
        "        for friend in friends:\n",
        "            # add each friend relation as an edge\n",
        "            edges.append((user_id, friend))\n",
        "    edges = sorted(edges)\n",
        "    \n",
        "    return edges\n",
        "\n",
        "def load_test_data(file_name):\n",
        "    \"\"\"\n",
        "    read edges from an edge file\n",
        "    \"\"\"\n",
        "    edges = list()\n",
        "    scores = list()\n",
        "    df = pd.read_csv(file_name)\n",
        "    for idx, row in df.iterrows():\n",
        "        edges.append((row[\"src\"], row[\"dst\"]))\n",
        "    edges = sorted(edges)\n",
        "    \n",
        "    return edges\n",
        "\n",
        "def generate_false_edges(true_edges, num_false_edges=5):\n",
        "    \"\"\"\n",
        "    generate false edges given true edges\n",
        "    \"\"\"\n",
        "    nodes = list(set(chain.from_iterable(true_edges)))\n",
        "    N = len(nodes)\n",
        "    true_edges = set(true_edges)\n",
        "    print(N, len(true_edges))\n",
        "    false_edges = set()\n",
        "    \n",
        "    while len(false_edges) < num_false_edges:\n",
        "        # randomly sample two different nodes and check whether the pair exisit or not\n",
        "        src, dst = nodes[int(np.random.rand() * N)], nodes[int(np.random.rand() * N)]\n",
        "        if src != dst and (src, dst) not in true_edges and (src, dst) not in false_edges:\n",
        "            false_edges.add((src, dst))\n",
        "    false_edges = sorted(false_edges)\n",
        "    \n",
        "    return false_edges\n",
        "\n",
        "def construct_graph_from_edges(edges):\n",
        "    \"\"\"\n",
        "    generate a directed graph object given true edges\n",
        "    DiGraph documentation: https://networkx.github.io/documentation/stable/reference/classes/digraph.html\n",
        "    \"\"\"\n",
        "    # convert a list of edges {(u, v)} to a list of edges with weights {(u, v, w)}\n",
        "    edge_weight = defaultdict(float)\n",
        "    for e in edges:\n",
        "        edge_weight[e] += 1.0\n",
        "    weighed_edge_list = list()\n",
        "    for e in sorted(edge_weight.keys()):\n",
        "        weighed_edge_list.append((e[0], e[1], edge_weight[e]))\n",
        "        \n",
        "    graph = nx.DiGraph()\n",
        "    graph.add_weighted_edges_from(weighed_edge_list)\n",
        "    \n",
        "    print(\"number of nodes:\", graph.number_of_nodes())\n",
        "    print(\"number of edges:\", graph.number_of_edges())\n",
        "    \n",
        "    return graph\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7dfNP0pxpgK"
      },
      "source": [
        "\n",
        "def alias_setup(probs):\n",
        "    \"\"\"\n",
        "    compute utility lists for non-uniform sampling from discrete distributions.\n",
        "    details: https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
        "    \"\"\"\n",
        "    K = len(probs)\n",
        "    q = np.zeros(K)\n",
        "    J = np.zeros(K, dtype=np.int)\n",
        "\n",
        "    smaller = list()\n",
        "    larger = list()\n",
        "    for kk, prob in enumerate(probs):\n",
        "        q[kk] = K * prob\n",
        "        if q[kk] < 1.0:\n",
        "            smaller.append(kk)\n",
        "        else:\n",
        "            larger.append(kk)\n",
        "\n",
        "    while len(smaller) > 0 and len(larger) > 0:\n",
        "        small = smaller.pop()\n",
        "        large = larger.pop()\n",
        "\n",
        "        J[small] = large\n",
        "        q[large] = q[large] + q[small] - 1.0\n",
        "        if q[large] < 1.0:\n",
        "            smaller.append(large)\n",
        "        else:\n",
        "            larger.append(large)\n",
        "\n",
        "    return J, q\n",
        "\n",
        "def get_alias_node(graph, node):\n",
        "    \"\"\"\n",
        "    get the alias node setup lists for a given node.\n",
        "    \"\"\"\n",
        "    # get the unnormalized probabilities with the first-order information\n",
        "    unnormalized_probs = list()\n",
        "    for nbr in graph.neighbors(node):\n",
        "        unnormalized_probs.append(graph[node][nbr][\"weight\"])\n",
        "    unnormalized_probs = np.array(unnormalized_probs)\n",
        "    if len(unnormalized_probs) > 0:\n",
        "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
        "    else:\n",
        "        normalized_probs = unnormalized_probs\n",
        "        \n",
        "    return alias_setup(normalized_probs)\n",
        "    \n",
        "def get_alias_edge(graph, src, dst, p=1, q=1):\n",
        "    \"\"\"\n",
        "    get the alias edge setup lists for a given edge.\n",
        "    \"\"\"\n",
        "    # get the unnormalized probabilities with the second-order information\n",
        "    unnormalized_probs = list()\n",
        "    for dst_nbr in graph.neighbors(dst):\n",
        "        if dst_nbr == src: # distance is 0\n",
        "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/p)\n",
        "        elif graph.has_edge(dst_nbr, src): # distance is 1\n",
        "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"])\n",
        "        else: # distance is 2\n",
        "            unnormalized_probs.append(graph[dst][dst_nbr][\"weight\"]/q)\n",
        "    unnormalized_probs = np.array(unnormalized_probs)\n",
        "    if len(unnormalized_probs) > 0:\n",
        "        normalized_probs = unnormalized_probs / unnormalized_probs.sum()\n",
        "    else:\n",
        "        normalized_probs = unnormalized_probs\n",
        "\n",
        "    return alias_setup(normalized_probs)\n",
        "\n",
        "def preprocess_transition_probs(graph, p=1, q=1):\n",
        "    \"\"\"\n",
        "    preprocess transition probabilities for guiding the random walks.\n",
        "    \"\"\"\n",
        "    alias_nodes = dict()\n",
        "    for node in graph.nodes():\n",
        "        alias_nodes[node] = get_alias_node(graph, node)\n",
        "\n",
        "    alias_edges = dict()\n",
        "    for edge in graph.edges():\n",
        "        alias_edges[edge] = get_alias_edge(graph, edge[0], edge[1], p=p, q=q)\n",
        "\n",
        "    return alias_nodes, alias_edges"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uur9Z0yx7VE"
      },
      "source": [
        "def alias_draw(J, q):\n",
        "    \"\"\"\n",
        "    draw sample from a non-uniform discrete distribution using alias sampling.\n",
        "    \"\"\"\n",
        "    K = len(J)\n",
        "\n",
        "    kk = int(np.floor(np.random.rand() * K))\n",
        "    if np.random.rand() < q[kk]:\n",
        "        return kk\n",
        "    else:\n",
        "        return J[kk]\n",
        "\n",
        "\n",
        "# helper function to generate the long random walk as desired\n",
        "def fallback(walk, fetch_last_num=1):\n",
        "    if len(walk) > fetch_last_num:\n",
        "        walk.pop()\n",
        "        fetched = []\n",
        "        for i in range(fetch_last_num):\n",
        "            fetched.append(walk[-1-i])\n",
        "        return walk, fetched\n",
        "    else:\n",
        "        return [], [None for _ in range(fetch_last_num)]\n",
        "\n",
        "def generate_first_order_random_walk(graph, alias_nodes, \n",
        "                                     walk_length=10, start_node=None, verbose=False, max_trails=10):\n",
        "    \"\"\"\n",
        "    simulate a random walk starting from start node and considering the first order information.\n",
        "    max_trials: set the max trials to be one for standard random walk. Larger max_trails will make the generated biased.\n",
        "    \"\"\"\n",
        "    if start_node == None:\n",
        "        start_node = np.random.choice(graph.nodes())\n",
        "    walk = [start_node]\n",
        "    cur = start_node\n",
        "    num_tried = 0\n",
        "    \n",
        "    ########## begin ##########\n",
        "    while len(walk) < walk_length:\n",
        "        cur_nbrs = list(graph.neighbors(cur))\n",
        "        if len(cur_nbrs) > 0: # if we can sample next nodes\n",
        "            # sample the next node based on alias_nodes\n",
        "            cur = cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
        "            walk.append(cur)\n",
        "        else: # if we can't do that\n",
        "            num_tried += 1\n",
        "            if num_tried >= max_trails:\n",
        "                break\n",
        "\n",
        "            walk, fetched = fallback(walk, fetch_last_num=1)\n",
        "            cur = fetched[0]\n",
        "            if len(walk) == 0: # if falls back to the empty walk\n",
        "                start_node = np.random.choice(graph.nodes())\n",
        "                walk = [start_node]\n",
        "                cur = start_node\n",
        "    ########## end ##########\n",
        "\n",
        "    if verbose: \n",
        "        print(f'walk of lenght {len(walk)} generated with {num_tried} trails')\n",
        "    return walk\n",
        "\n",
        "def generate_second_order_random_walk(graph, alias_nodes, alias_edges, \n",
        "                                      walk_length=10, start_node=None, verbose=False, max_trails=10):\n",
        "    \"\"\"\n",
        "    simulate a random walk starting from start node and considering the second order information.\n",
        "    \"\"\"\n",
        "    if start_node == None:\n",
        "        start_node = np.random.choice(graph.nodes())\n",
        "    walk = [start_node]\n",
        "    \n",
        "    prev = None\n",
        "    cur = start_node\n",
        "    num_tried = 0\n",
        "\n",
        "    ########## begin ##########\n",
        "    while len(walk) < walk_length:\n",
        "        cur_nbrs = list(graph.neighbors(cur))\n",
        "        if len(cur_nbrs) > 0:\n",
        "            if prev is None:\n",
        "                # sample the next node based on alias_nodes\n",
        "                prev, cur = cur, cur_nbrs[alias_draw(*alias_nodes[cur])]\n",
        "            else:\n",
        "                # sample the next node based on alias_edges\n",
        "                prev, cur = cur, cur_nbrs[alias_draw(*alias_edges[(prev, cur)])]\n",
        "            walk.append(cur)\n",
        "        else:\n",
        "            num_tried += 1\n",
        "            if num_tried >= max_trails:\n",
        "                break\n",
        "            walk, (cur, prev) = fallback(walk, fetch_last_num=2)\n",
        "            if len(walk) == 0:\n",
        "                start_node = np.random.choice(graph.nodes())\n",
        "                walk = [start_node]\n",
        "                cur = start_node\n",
        "                prev = None\n",
        "    ########## end ##########\n",
        "    if verbose: \n",
        "        print(f'walk of lenght {len(walk)} generated with {num_tried} trails')\n",
        "    return walk"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI8fCQF8yHg-"
      },
      "source": [
        "def build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10):\n",
        "    \"\"\"\n",
        "    build a deepwalk model\n",
        "    \"\"\"\n",
        "    print(\"building a DeepWalk model...\", end=\"\\t\")\n",
        "    st = time.time()\n",
        "    np.random.seed(0)\n",
        "    nodes = list(graph.nodes())\n",
        "    walks = list()\n",
        "    # generate random walks\n",
        "    for walk_iter in range(num_walks):\n",
        "        np.random.shuffle(nodes)\n",
        "        for node in nodes:\n",
        "            walks.append(generate_first_order_random_walk(\n",
        "                graph, alias_nodes, walk_length=walk_length, start_node=node))\n",
        "        \n",
        "    walk_lens = [len(w) for w in walks]\n",
        "    if len(walk_lens) > 0:\n",
        "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
        "    else:\n",
        "        avg_walk_len = 0.0\n",
        "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
        "    \n",
        "    # train a skip-gram model for these walks\n",
        "    model = Word2Vec(walks, size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), iter=10) #vector_ epochs\n",
        "    print(\"training time: %.4f\" % (time.time()-st))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10):\n",
        "    \"\"\"\n",
        "    build a node2vec model\n",
        "    \"\"\"\n",
        "    print(\"building a node2vec model...\", end=\"\\t\")\n",
        "    st = time.time()\n",
        "    np.random.seed(0)\n",
        "    nodes = list(graph.nodes())\n",
        "    walks = list()\n",
        "    # generate random walks\n",
        "    for walk_iter in range(num_walks):\n",
        "        np.random.shuffle(nodes)\n",
        "        for node in nodes:\n",
        "            walks.append(generate_second_order_random_walk(\n",
        "                graph, alias_nodes, alias_edges, walk_length=walk_length, start_node=node))\n",
        "            \n",
        "    walk_lens = [len(w) for w in walks]\n",
        "    if len(walk_lens) > 0:\n",
        "        avg_walk_len = sum(walk_lens) / len(walk_lens)\n",
        "    else:\n",
        "        avg_walk_len = 0.0    \n",
        "    print(\"number of walks: %d\\taverage walk length: %.4f\" % (len(walks), avg_walk_len), end=\"\\t\")\n",
        "    \n",
        "    # train a skip-gram model for these walks\n",
        "    model = Word2Vec(walks, size=node_dim, window=3, min_count=0, sg=1, workers=os.cpu_count(), iter=10) #vector_ epochs\n",
        "    print(\"training time: %.4f\" % (time.time()-st))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jleX-9uWyND0"
      },
      "source": [
        "def get_cosine_sim(model, u, v):\n",
        "    \"\"\"\n",
        "    get the cosine similarity between two nodes\n",
        "    \"\"\"\n",
        "    try:\n",
        "        u = model.wv[u]\n",
        "        v = model.wv[v]\n",
        "        return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "    except:\n",
        "        return 0.5\n",
        "\n",
        "def get_auc_score(model, true_edges, false_edges):\n",
        "    \"\"\"\n",
        "    get the auc score\n",
        "    \"\"\"\n",
        "    y_true = [1] * len(true_edges) + [0] * len(false_edges)\n",
        "    \n",
        "    y_score = list()\n",
        "    for e in true_edges:\n",
        "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
        "    for e in false_edges:\n",
        "        y_score.append(get_cosine_sim(model, e[0], e[1]))\n",
        "    \n",
        "    return roc_auc_score(y_true, y_score)\n",
        "\n",
        "def write_pred(file_name, edges, scores):\n",
        "    df = pd.DataFrame()\n",
        "    df[\"src\"] = [e[0] for e in edges]\n",
        "    df[\"dst\"] = [e[1] for e in edges]\n",
        "    df[\"score\"] = scores\n",
        "    df.to_csv(file_name, index=False)\n",
        "\n",
        "def write_valid_ans(file_name, edges, scores):\n",
        "    df = pd.DataFrame()\n",
        "    df[\"src\"] = [e[0] for e in edges]\n",
        "    df[\"dst\"] = [e[1] for e in edges]\n",
        "    df[\"score\"] = scores\n",
        "    df.to_csv(file_name, index=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6NTWH9mzNls",
        "outputId": "3f62aa62-8fd2-4da9-9176-d28afa3ce014"
      },
      "source": [
        "train_file = \"/content/gdrive/MyDrive/COMP4332PA2/train.csv\"\n",
        "valid_file = \"/content/gdrive/MyDrive/COMP4332PA2/valid.csv\"\n",
        "test_file = \"/content/gdrive/MyDrive/COMP4332PA2/test.csv\"\n",
        "\n",
        "np.random.seed(0)\n",
        "train_edges = load_data(train_file)\n",
        "graph = construct_graph_from_edges(train_edges)\n",
        "valid_edges = load_data(valid_file)\n",
        "false_edges = generate_false_edges(train_edges+valid_edges, 40000-len(valid_edges))\n",
        "test_edges = load_test_data(test_file)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of nodes: 8328\n",
            "number of edges: 100000\n",
            "8474 119268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXgPU0sCz7vE"
      },
      "source": [
        "alias_nodes, alias_edges = preprocess_transition_probs(graph, p=1, q=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOjkdSj6z_XW",
        "outputId": "a704ddb4-9bd7-460b-9f6d-d3d87b978e30"
      },
      "source": [
        "generate_first_order_random_walk(graph, alias_nodes=alias_nodes,\n",
        "                                 start_node=\"N6ZTMIue-2b30CJv2tyPGg\", walk_length=10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N6ZTMIue-2b30CJv2tyPGg',\n",
              " 'aKhv49qQ1A1MDvGZ-cVh7Q',\n",
              " 'maK3UBQczh33NuDjBYeHrA',\n",
              " '2MLTkPNp9ldB4YBOi5k4qA',\n",
              " '5G7dOTrKQrbyFG8fpBnx7g',\n",
              " 'ZbeknMQQmKosOCRhu9utHg',\n",
              " 'M9JDlUWudvrGkLX7AkJ3Fw',\n",
              " '7dnh9OrD8LekBwvachGH5Q',\n",
              " 'Db80SCez0BdYaHdC0xmzoQ',\n",
              " 'om5ZiponkpRqUNa3pVPiRg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClZbNIjr0Dik",
        "outputId": "e7657a95-6511-4311-bcc6-80a844ba950e"
      },
      "source": [
        "generate_second_order_random_walk(graph, alias_nodes=alias_nodes, alias_edges=alias_edges,\n",
        "                                  start_node=\"N6ZTMIue-2b30CJv2tyPGg\", walk_length=10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N6ZTMIue-2b30CJv2tyPGg',\n",
              " 'OPHtdryCpZaooJ_kfJlDxA',\n",
              " 'N7E-CfqdME28dakWdEKNvw',\n",
              " 'dIIKEfOgo0KqUfGQvGikPg',\n",
              " '3nDUQBjKyVor5wV0reJChg',\n",
              " 'jRyO2V1pA4CdVVqCIOPc1Q',\n",
              " '0FMte0z-repSVWSJ_BaQTg',\n",
              " 'cQzo2WX1TyPEmS5jPzOUoA',\n",
              " 'U4INQZOPSUaj8hMjLlZ3KA',\n",
              " 'RBZ_kMjowV0t6_nv2UKaDQ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trzdr-6h0HLt",
        "outputId": "ddb9c52a-1892-47c1-96e8-925efb6a3cb8"
      },
      "source": [
        "model = build_deepwalk(graph, alias_nodes, node_dim=10, num_walks=10, walk_length=10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building a DeepWalk model...\tnumber of walks: 83280\taverage walk length: 9.8980\ttraining time: 31.9807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdIT7JgM0KgD",
        "outputId": "d9cc9d6c-ebc7-4a02-947e-4fea8d0c6a30"
      },
      "source": [
        "model = build_node2vec(graph, alias_nodes, alias_edges, node_dim=10, num_walks=10, walk_length=10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building a node2vec model...\tnumber of walks: 83280\taverage walk length: 9.9910\ttraining time: 34.8385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vBvqrJ84z-b",
        "outputId": "f7cf68a7-d908-4412-ef09-84f2d29086b4"
      },
      "source": [
        "np.random.seed(0)\n",
        "deepwalk_auc_scores = dict()\n",
        "\n",
        "node_dims = range(1, 11)\n",
        "nums_walks =range(1, 11)\n",
        "walk_lengths = range(1, 11)\n",
        "# times = 1\n",
        "for node_dim in node_dims:\n",
        "    for num_walks in nums_walks:\n",
        "        for walk_length in walk_lengths:\n",
        "            # print(str(times)+'. training time')\n",
        "            # print(\"node dim: %d,\\tnum_walks: %d,\\twalk_length: %d\" % (node_dim, num_walks, walk_length), end=\"\\t\")\n",
        "            model = build_deepwalk(graph, alias_nodes, node_dim=node_dim, num_walks=num_walks, walk_length=walk_length)\n",
        "            deepwalk_auc_scores[(node_dim, num_walks, walk_length)] = get_auc_score(model, valid_edges, false_edges)\n",
        "            # print(\"auc: %.4f\" % (deepwalk_auc_scores[(node_dim, num_walks, walk_length)]))\n",
        "            # times = times + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 1.0000\ttraining time: 1.9126\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 2.0000\ttraining time: 2.6320\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 2.9882\ttraining time: 2.8680\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 3.9762\ttraining time: 3.1168\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 4.9631\ttraining time: 3.3349\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 5.9496\ttraining time: 3.5562\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 6.9378\ttraining time: 3.7231\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 7.9247\ttraining time: 3.9100\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 8.9120\ttraining time: 4.1487\n",
            "building a DeepWalk model...\tnumber of walks: 8328\taverage walk length: 9.8983\ttraining time: 4.4196\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 1.0000\ttraining time: 1.9015\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 2.0000\ttraining time: 3.8639\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 2.9882\ttraining time: 4.1385\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 3.9754\ttraining time: 4.5245\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 4.9629\ttraining time: 5.0168\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 5.9506\ttraining time: 5.3889\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 6.9377\ttraining time: 5.8252\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 7.9247\ttraining time: 6.4123\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 8.9100\ttraining time: 6.6303\n",
            "building a DeepWalk model...\tnumber of walks: 16656\taverage walk length: 9.8990\ttraining time: 7.1426\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 1.0000\ttraining time: 2.0468\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 2.0000\ttraining time: 4.7803\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 2.9884\ttraining time: 5.6099\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 3.9760\ttraining time: 5.9156\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 4.9623\ttraining time: 6.7087\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 5.9510\ttraining time: 7.2277\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 6.9374\ttraining time: 8.1116\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 7.9243\ttraining time: 8.6649\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 8.9111\ttraining time: 9.1815\n",
            "building a DeepWalk model...\tnumber of walks: 24984\taverage walk length: 9.8990\ttraining time: 9.9010\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 1.0000\ttraining time: 2.1615\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 2.0000\ttraining time: 5.9587\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 2.9884\ttraining time: 6.7321\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 3.9760\ttraining time: 7.6001\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 4.9617\ttraining time: 8.3931\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 5.9510\ttraining time: 9.2622\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 6.9373\ttraining time: 10.1472\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 7.9239\ttraining time: 10.9324\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 8.9122\ttraining time: 11.9370\n",
            "building a DeepWalk model...\tnumber of walks: 33312\taverage walk length: 9.8976\ttraining time: 12.4525\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 1.0000\ttraining time: 2.3274\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 2.0000\ttraining time: 6.8987\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 2.9884\ttraining time: 8.0613\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 3.9760\ttraining time: 9.0169\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 4.9622\ttraining time: 10.0748\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 5.9510\ttraining time: 11.2935\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 6.9371\ttraining time: 12.3337\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 7.9238\ttraining time: 13.3334\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 8.9126\ttraining time: 14.3213\n",
            "building a DeepWalk model...\tnumber of walks: 41640\taverage walk length: 9.8976\ttraining time: 15.3490\n",
            "building a DeepWalk model...\tnumber of walks: 49968\taverage walk length: 1.0000\ttraining time: 2.4757\n",
            "building a DeepWalk model...\tnumber of walks: 49968\taverage walk length: 2.0000\ttraining time: 8.1054\n",
            "building a DeepWalk model...\tnumber of walks: 49968\taverage walk length: 2.9884\ttraining time: 9.6427\n",
            "building a DeepWalk model...\t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARbeNrqpC1_U"
      },
      "source": [
        "acc_max = max(list(deepwalk_auc_scores.values()))\n",
        "for sets, acc in deepwalk_auc_scores.items():\n",
        "    if acc == acc_max:\n",
        "      print(sets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}